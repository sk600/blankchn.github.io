<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        机器学习（3）神经网络初探 · sk600&#39;s Studio
        
    </title>
    <link rel="icon" href= /assets/touxian.png>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171227 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <script>
        var _hmt = _hmt || [];
        (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?true";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
    <!-- 谷歌统计  -->
    
    <script>
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'true', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >sk600&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">机器学习（3）神经网络初探</a>
            </div>
    </div>
    
    <a class="home-link" href=/>sk600's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            机器学习（3）神经网络初探
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = 机器学习>机器学习</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = 学习笔记>学习笔记</a>
    
</div>
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/10/05</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <blockquote>
<p><a href="https://www.bilibili.com/video/av9912938" target="_blank" rel="noopener">bilibili课程地址</a>,<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">coursera地址</a>。</p>
</blockquote>
<h1 id="神经网络初探"><a href="#神经网络初探" class="headerlink" title="神经网络初探"></a>神经网络初探</h1><p>不像逻辑回归那样只有一层$\theta$的映射，神经网络把模型的计算输出划分为多层，从输入层到隐藏层（可能有多层隐藏层），最后到最后的输出层。每一层包含多个计算端点（神经元），每一个计算端点只负责一个逻辑回归的计算，后面层的计算单元将前面层计算单元的输出当作输入，层层递进，最终达到最后的输出端点。每一个输出端点作为一个分类，通过输出类似$\begin{bmatrix} {0}\ {0}\ {1}\ {0}\ {0}\ \end{bmatrix}$的向量表示类别判断结果（示例表示该输入的最终结果是第三类）。</p>
<p><img src="/images/image-20181006153511577.png" alt="image-20181006153511577"></p>
<h2 id="神经网络的算法逻辑"><a href="#神经网络的算法逻辑" class="headerlink" title="神经网络的算法逻辑"></a>神经网络的算法逻辑</h2><p><img src="/images/image-20181006140355743.png" alt="image-20181006140355743"></p>
<p>如上图所示，这是一个最简单的神经网络，只有三层，其中$x_1^{(i)},x_2^{(i)}$为输入（第一层），中间两层表示隐藏层，最后一个表示输出层,中间的每一个$\theta$ 是每一层向下一层传递时的权重，比如 $w_1,w_2$ 就是 $i_1,i_2$ 向 $h_1$传递的权重，1是偏置项。</p>
<h3 id="第一步-前向传播"><a href="#第一步-前向传播" class="headerlink" title="第一步 前向传播"></a>第一步 前向传播</h3><p>针对于每一个输入输出 ${x^(i),y^(i)}$    都有 $x^{(i)} = \begin{bmatrix} {x_1^{(i)}}\ {x_2^{(i)}}\ \end{bmatrix} $ ，还要加一个+1的偏置项（就像逻辑回归中一样），第二层，也就是隐藏层第一层 的输入用 $z^{(2)}$  （对于隐藏层和输出层来说2表示的是第二层，z表示输入） ，$z^{(2)} = \begin{bmatrix} {z_1^{(2)}}\ {z_2^{(2)}}\ \end{bmatrix}$ ，$z_i^{(l)}$ 表示第l层的第i个神经元的输入，第三层同理。</p>
<p>$z_i^{(k)}$的计算如下所示：</p>
<script type="math/tex; mode=display">
z_1^{(2)} = \theta_{01}*1+\theta_{11} x_1^{(i)} + \theta_{12} x_2^{(i)}\\
\theta_{ij} 表示前一层的第j个神经元向下一层的第i个神经元传递时的权重</script><p> 每一个神经元里面也需要进行计算，计算方法与逻辑回归的 $h_{\theta}(z)$一样：</p>
<script type="math/tex; mode=display">
a_1^{(2)} = \frac {1}{1+e^{-z_1^{(2)}}}\\
a_i^l 表示第l层的第i个神经元的输出</script><p>相当于在每一个神经元里面做一次逻辑回归的迭代（当然这里还没有修改参数）。$z_2^{(2)}$也是同样的计算规则，接着同第二层的偏置项（+1）再向第三层进行传递，计算方法一样，这样就计算出了 $a^{(3)}$ （第三层的输出）， 再用 $a^{(3)}$计算出 $z^{(4)},a^{(4)}$ , $a^{(4)}$ 就是这个神经网络的输出了，当然这里的输出神经元只有一个，如果有多个就输出成向量的形式。</p>
<p>正向传播中的流程主要是这样的:</p>
<center>

<div id="flowchart-0" class="flow-chart"></div>

</center>

<h3 id="第二步反向传播"><a href="#第二步反向传播" class="headerlink" title="第二步反向传播"></a>第二步反向传播</h3><p>机器学习最神奇的就是通过对数据集的计算以及结果的对比能够自动调整参数，进而达到自动优化模型的结果，神经网络也是一样的，它的调整参数的方法就是通过反向传播进行。</p>
<p>通过上述正向传播过程，可以发现唯一能够调整的过程就是通过上一层的 $a^{(l-1)}$ 计算 $a^{(l)}$  中的 $\theta$ ，所以反向传播中的主要目的就是调整 $\theta$ 。</p>
<p>因为 $\theta$ 的误差主要会传递到 $z,a$ 继而传递到最后的输出，而这也是唯一能够直接找到误差所在的地方，所以正向传播的结果就是反向传播的起点，这也是反向传播的名字意义（我自己这么理解的）。</p>
<p>代价函数如下：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}(\sum_{i=1}^{m} \sum_{k=1}^{k} y_k^{(i)}log(h_{\theta}(x^{(i)}))_k +(1-y_k^{(i)})log(1-h_{\theta}(x^{(i)}))_k) + \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l+1}}\sum_{j=1}^{s_l}{\theta_{ij}^{(l)}}^2\\
y_k^{(i)} 表示对于第i个训练数据的第k个输出单元输出结果\\
m表示训练集的数量\\
 \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l+1}}\sum_{j=1}^{s_l}{\theta_{ij}^{(l)}}^2 这是一个正则项，防止过拟合</script><p> $\frac {\partial J(\theta)}{\partial \theta<em>{ij}^{(k)}} $ 就是每一个 $\theta</em>{ij}^{(k)}$</p>
<p>对于输出层的误差非常好找：</p>
<script type="math/tex; mode=display">
\delta^{(4)} = a^{(4)} - y^{(i)} \\
\delta^{(k)}表示第k层（相对于z^{(k)}的误差）</script><p>对于隐藏层的误差，公式如下：</p>
<script type="math/tex; mode=display">
\delta^{(l)} = \theta^{(k)}\delta^{(l+1)}g^{'}(z^{(l)})\\
\theta^{(l)} 表示第k层神经元向第k+1层神经元传递时的权重\\
g^{'}(x) = g(x)(1-g(x)) \quad\quad g(x) = \frac {1}{1+e^{-x}}</script><p>后面会推到这个 $\delta^{(l)}$ ，接下来就是找到这个 $\delta^{(l)}$ 与 $\theta^{(l-1)}$ 的关系：</p>
<script type="math/tex; mode=display">
\frac {\partial {J({\theta})}}{\partial {\theta_{ij}^{(l)}}} = a_j^{(l)}\delta_i^{l+1}\\</script><p>因为在对训练集中m组数据训练完成以后才要进行反向传播，并且每一组数据所得到的 $\frac {\partial J(\theta)} {\partial \theta_{ij}^{(l)}}$ 都是不一样的（即，每一组数据是有自己特征的，模型并不能够对每一组数据产生一样的误差，如果产生的一样的误差的话，那误差就可以直接改掉，那模型不就是完全契合数据了 ——过拟合，<strong>这是我的理解</strong>），所以需要求一个平均的影响（平均在机器学习中用的非常普遍，感觉只要涉及到不一致，那么就会求和平均来求平衡，我觉得理解平均的含义是非常重要的多，总感觉现在对于平均的理解还很肤浅，之后要看一看，这样理解起来就能更加深刻一些了），这个求和的变量就用 $\Delta$ 表示：</p>
<script type="math/tex; mode=display">
\Delta_{ij}^{(l)} = \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}\\
\Delta_{ij}^{(l)}表示对于m组训练数据针对于\theta_{ij}^{(l)}的误差影响总和，这里不是误差总和，因为\Delta是偏导数\\</script><p> 有了影响总和，那么就可以对计算$\frac {\partial J(\theta)} {\partial \theta_{ij}^{(l)}}$ 了，用 $D$ 表示：</p>
<script type="math/tex; mode=display">
D_{ij}^{(l)} = \frac {1} {m} \Delta_{ij}^{(l)} +  \underbrace{\lambda \theta_{ij}^{(l)}}_{\lambda \theta_{ij}^{(l)}  这是对应 J(\theta) 的正则项对于 \theta_{ij}^{(l)}的求导。} \quad j\not=0\\
D_{ij}^{(l)} = \frac {1} {m} \Delta_{ij}^{(l)} \quad j=0\\</script><p>为什么在j=0的情况下不接入这个，这是因为j=0对应的是偏置项，偏置项在逻辑回归和线性回归中也是不进行正则化计算的（具体原因还不太理解，但是我试了一下，同样的迭代次数，准确性反而下降了），接下来就是对  $ \theta_{ij}^{(l)} $ 进行调整了:</p>
<script type="math/tex; mode=display">
\theta_{ij}^{(l)} = \theta_{ij}^{l} + \alpha D_{ij}^{(l)}\\
\alpha 表示学习率</script><p>反向传播流程如下：</p>
<center>

<div id="flowchart-1" class="flow-chart"></div>

</center>

<p><strong>附加推导</strong></p>
<ul>
<li>对于隐藏层 $\delta_{i}^{(l)}$的推导：</li>
</ul>
<script type="math/tex; mode=display">
证明：\delta_i^{(l)} = {\theta_i^{(l)}}^T \delta^{(l+1)}g^{'}(z^{(l)}) \quad (\delta_i^{(l)}=\frac {\partial J(\theta)} {\partial z_i^{(l)}} \quad 表示第l层第i个神经元对于整体代价误差的影响） \\
\because \delta_i^{(l)} = \frac {\partial J(\theta)} {\partial a_i^{(l)}} \frac {\partial a_i^{(l)}} {\partial z_i^{(l)}}\\
且 \because a_i^{(l)} = g(z_i^{(l)})\\
\therefore \frac {\partial a_i^{(l)}} {\partial z_i^{(l)}} = g^{'}(z_i^{(l)})\\
\because \frac {\partial J(\theta)} {\partial a_i^{(l)}} = \sum_{j=1}^{s_{l+1}} \frac {\partial J(\theta)} {\partial z_j^{(l+1)}} \frac {\partial z_j^{(l+1)}} {\partial a_i^{(l)}} \quad (1) \quad s_{l+1}表示第l+1层神经元的数量\\
又\because a_i^{(l)} \theta_{ji}^{(l)} = z_j^{(l+1)}\\
\frac {\partial J(\theta)} {\partial z_j^{(l+1)}} = \delta_j^{(l+1)}\\
\therefore (1)式=\sum_{j=1}^{s_{l+1}} \frac {\partial J(\theta)} {\partial z_j^{(l+1)}} \theta_{ji}^{(l)}\\
=\sum_{j=1}^{s_{l+1}} \delta_j^{(l+1)} \theta_{ji}^{(l+1)}\\
={\theta_i^{(l)}}^T \delta^{(l+1)}\\
注意：\theta_i^{(l)} 表示第l层神经元对于第l+1层所有神经元的权重向量\\
\delta^{(l+1)} = \begin{bmatrix} {\delta_1^{(l+1)}}\\ {\delta_2^{(l+1)}}\\ {...}\\ {\delta_{s_{l+1}}^{(l+1)}} \end{bmatrix}\\
证得：\delta_i^{(l)} = {\theta_i^{(l)}}^T \delta^{(l+1)}g^{'}(z^{(l)})\\</script><ul>
<li>对于 $\Delta_{ij}^{(l)}$ 的推导：<script type="math/tex; mode=display">
证明:\Delta_{ij}^{(l)} = \sum_{k=1}^{m} a_j^{(l)} \delta_{i}^{(l+1)} \quad \\
(注意：原式为\Delta_{ij}^{(l)} = \Delta_{ij}^{(l)} + a_j^{(l)} \delta_{i}^{(l+1)}，\\
这其实是一种计算机式的表达方式，就是对每一个测试数据的\Delta_{ij}^{(l)}进行累加，也就是上述的样子，\\
所以这里也就是证明对于每一个测试数据的\Delta_{ij}^{(l)} = a_j^{(l)} \delta_{i}^{(l+1)})\\
\Delta_{ij}^{(l)} = \frac {\partial J(\theta)} {\partial z_i^{(l+1)}} \frac {\partial z_i^{(l+1)}} {\partial \theta_{ij}^{(l)}}\\
\because \frac {\partial J(\theta)} {\partial z_i^{(l+1)}} = \delta_i^{(l+1)}\\
\because a_j^{(l)} \theta_{ij}^{(l)} = z_i^{(l+1)}\\
\therefore \frac {\partial z_i^{(l+1)}} {\partial \theta_{ij}^{(l)}} = a_j^{(l)}\\
证得：\Delta_{ij}^{(l)} = a_j^{(l)} \delta_{i}^{(l+1)}\\</script></li>
</ul>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>之前已经将神经网络的计算过程已经初步介绍了一遍，但是中间还会遇到一些细节的问题导致模型的失败，对于 $\theta$ 的初始化设置就是其中之一。</p>
<p><img src="/images/image-20181008001627966.png" alt="image-20181008001627966"> </p>
<p>这里用视频中的一张截图来说明初始化的问题，看红线和青色的线，如果一开始设置的值都是一样的：</p>
<script type="math/tex; mode=display">
设上面青色的线为\theta_{11}^{(2)}，下面青色的线为\theta_{12}^{(2)}，输出神经层神经元的输入、输出分别为z_1^{(3)}、a_1^{(3)}\\
那么\frac {\partial J(\theta)} {\partial \theta_{11}^{(2)}} = \frac {\partial J(\theta)} {\partial z_1^{(3)}} \frac {\partial z_1^{(3)}} {\partial \theta_{11}^{(2)}}\\
=\delta_1^{(3)} a_1^{(2)}\quad (即 \Delta_{11}^{(2)})\\
且\Delta_{12}^{(3)} = \delta_1^{(3)} a_2^{(2)}\\
又\because a_1^{(2)} = 1·\theta_{10}^{(1)} + x_1 \theta_{11}^{(1)} + x_2 \theta_{12}^{(1)}\\
a_2^{(2)} = 1·\theta_{20}^{(1)} + x_1 \theta_{21}^{(1)} + x_2 \theta_{22}^{(1)}\\
又\because \theta 都相同\\
\therefore a_1^{(2)} = a_2^{(2)}\\
\therefore \Delta_{12}^{(2)} = \Delta_{11}^{(2)}\\
既然同一层的\Delta都相同，那么对于同一层\theta的修改相同，所以每一层的\theta都将永远保持不变</script><p>通过上述证明，$\theta$ 需要在初始化的时候进行一定的变化 （$\theta $ 一样，这样每一个神经元就不能有不同的作用，产出都一样那还要这么多神经元干什么，每一层保留一个将它的运行结果直接乘以数量不就好了 ，这是我的理解) ，以下提供了一种方法：</p>
<script type="math/tex; mode=display">
将\theta 初始化为[-\epsilon,\epsilon]的随机数就好了,具体做法如下：\\
(1) 生成一个随机的矩阵（每个值都在[0,1]之间)，矩阵的尺寸依照神经网络需\theta 而言（矩阵每一个位置都对应一个\theta.) \\
(2) 对于每一个值乘以2倍的\epsilon.（这时候的值\in [0,2\epsilon]）\\
(3) 再减去\epsilon.(这时候的值\in [-\epsilon,\epsilon])</script><h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>仔细观察神经网络，其实每一个神经元都是在做逻辑回归罢了，但是这个逻辑回归是一个传递性的，那么很可能在编写代码的时候一个小错误也随着传递影响到了全局（有时候即使错了，代价还是在递减的，但是达不到最优），所以需要一种检验方法——梯度检验。</p>
<p><img src="/images/image-20181008005532124.png" alt="image-20181008005532124"></p>
<p>梯度检验就是对每一个参数的 $\Delta$ 进行估计，如果估计值与神经网络的一样（误差非常小），那么就证明神经网络算法正确：</p>
<script type="math/tex; mode=display">
梯度检验就是针对每一个\theta 根据J(\theta)的值直接求一个近似\Delta ，然后进行比较,具体做法如下(以\theta_{11}^{(1)} 为例：\\
(1) 选取一个\epsilon ,使用同样的神经网络（参数一致）进行一次迭代 ，计算J(\theta_{10}^{(1)},\theta_{11}^{(1)}+\epsilon,...)，J(\theta_{10}^{(1)},\theta_{11}^{(1)}-\epsilon,...)\\
(2) 求{\Delta_{10}^{(1)}}_{temp} =  \frac {J(\theta_{10}^{(1)},\theta_{11}^{(1)}+\epsilon,...)-J(\theta_{10}^{(1)},\theta_{11}^{(1)}-\epsilon,...)} {2 \epsilon}\\
(3) 与原神经网络的\Delta_{11}^{(1)}进行比较，若误差非常小，则表示通过检验</script><p>梯度检验主要检测的是每一个参数是否沿着 $\frac {\partial J(\theta)} {\partial \theta}$ 梯度方向下降，保证朝着最优方向运动。 </p>
<p>但是梯度检验因为需要单独运行一次神经网络，所以不能每次迭代进行一次检验，只是在写完神经网络算法后运行一次迭代，然后通过梯度检验检验是否正确，如果正确则关闭梯度检验正式运行。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>神经网络是对于逻辑回归的一种扩展，只是这种扩展将原本一次性的计算推广到多层交叉计算，这样每一个神经元可能代表某种特殊的含义（相当于分解为多种变量），每一层的变量代表着一种层次的特征的抽象，然后不断递进向下一层抽象，抽象到输出层就表示这一层的特征已经满足分类所需了，如果准确率不高，增加层数准确率明显提升， 表明之前的特征还不能满足分类所需，还需要再进行抽象。当然这是我自己的理解，自我觉得神经网络算法是一个非常神奇并且挖掘潜力非常高的算法。  </p>
<p>神经网络算法的大致流程如下：</p>
<center>

<div id="flowchart-2" class="flow-chart"></div>

<p>&lt;/center&gt;<script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 开始
e=>end: 结束
input=>inputoutput: 输入X
output=>inputoutput: 输出向量
calZ=>operation: 计算Z(通过theta)
calA=>operation: 计算A
sub=>subroutine: 进入下一层
judge=>condition: 是否为输出层
st->input->calZ->calA->judge
judge(yes)->output->end
judge(no)->sub->calZ</textarea><textarea id="flowchart-0-options" style="display: none">"flowchart"</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><textarea id="flowchart-1-code" style="display: none">st=>start: 开始
e=>end: 结束
calJ=>operation: 计算J(theta)
caldel=>operation: 计算delta
calDel=>operation: 计算Delta
updatetheta=>operation: 更新theta
st->calJ->caldel->calDel->updatetheta</textarea><textarea id="flowchart-1-options" style="display: none">"flowchart"</textarea><script>  var code = document.getElementById("flowchart-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-1", options);</script><textarea id="flowchart-2-code" style="display: none">st=>start: 开始
e=>end: 结束
input=>inputoutput: 输入一组数据
output=>inputoutput: 输出分类结果
randomInit=>operation: 随机初始化
fp=>operation: 前向传播
bp=>operation: 反向传播
needChange=>condition: 梯度检验
change=>subroutine: 修改算法
calJ=>operation: 计算J(theta)
needJudge=>condition: 是否第一次运行
closeChange=>subroutine: 关闭梯度检验
needStop=>condition: 是否达到迭代次数
st->randomInit->input->needJudge
needJudge(yes)->needChange
needChange(no)->change->randomInit
needChange(yes)->closeChange->fp
needJudge(no)->fp->output->calJ->bp->needStop
needStop(yes)->e
needStop(no)->input</textarea><textarea id="flowchart-2-options" style="display: none">"flowchart"</textarea><script>  var code = document.getElementById("flowchart-2-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-2-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-2", options);</script></p>
</center>
    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
        
            <li class="previous">
                <a href= "/2018/09/18/机器学习（2）线性回归与逻辑回归的实现与思考/" title= 机器学习（2）线性回归与逻辑回归的实现与思考 >
                    <span>Previous Post</span>
                    <span>机器学习（2）线性回归与逻辑回归的实现与思考</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:13721100276@163.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/sk600" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
        <span id="busuanzi_container_site_pv">VISITOR VOLUME: <span id="busuanzi_value_site_pv"></span>
        </span>
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络初探"><span class="toc-number">1.</span> <span class="toc-text">神经网络初探</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的算法逻辑"><span class="toc-number">1.1.</span> <span class="toc-text">神经网络的算法逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步-前向传播"><span class="toc-number">1.1.1.</span> <span class="toc-text">第一步 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步反向传播"><span class="toc-number">1.1.2.</span> <span class="toc-text">第二步反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机初始化"><span class="toc-number">1.1.3.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度检验"><span class="toc-number">1.1.4.</span> <span class="toc-text">梯度检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">1.1.5.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 35 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/05</span><a class="archive-post-title" href= "/2018/10/05/机器学习（3）神经网络初探/" >机器学习（3）神经网络初探</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/18</span><a class="archive-post-title" href= "/2018/09/18/机器学习（2）线性回归与逻辑回归的实现与思考/" >机器学习（2）线性回归与逻辑回归的实现与思考</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2018/09/17/机器学习（1）逻辑回归与线性回归/" >机器学习（1）逻辑回归与线性回归</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/2018/09/13/线性代数（2）矩阵消元/" >线性代数（2）矩阵消元</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/12</span><a class="archive-post-title" href= "/2018/09/12/线性代数（1）方程组的解释/" >线性代数（1）方程组的解释</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/26</span><a class="archive-post-title" href= "/2018/07/26/统计学习方法（2）/" >统计学习方法（2）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span><a class="archive-post-title" href= "/2018/07/25/统计学习方法感想（1）/" >统计学习方法感想（1）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/18</span><a class="archive-post-title" href= "/2018/07/18/情景感知计算-物联网感想/" >情景感知计算-物联网感想</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span><a class="archive-post-title" href= "/2018/07/07/android-recyclerview超高headerview处理/" >android recyclerview超高headerview处理</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/21</span><a class="archive-post-title" href= "/2018/05/21/毕设感想（1）/" >毕设感想（1）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/16</span><a class="archive-post-title" href= "/2018/05/16/毕设云盘架构思路Eureka实现篇/" >毕设云盘架构思路Eureka实现篇</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/30</span><a class="archive-post-title" href= "/2018/03/30/记录财务app开发/" >记录财务app开发</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/28</span><a class="archive-post-title" href= "/2018/03/28/开启新的篇章/" >开启新的篇章</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/31</span><a class="archive-post-title" href= "/2018/01/31/重走android（12）OkHttp3源码解析（下）/" >重走android（12）OkHttp3源码解析（下）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/30</span><a class="archive-post-title" href= "/2018/01/30/重走android（11）OkHttp3源码解析（上）/" >重走android（11）OkHttp3源码解析（上）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/25</span><a class="archive-post-title" href= "/2018/01/25/重走android（10）ListView-RecycleBin机制/" >重走android（10）ListView RecycleBin机制</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/23</span><a class="archive-post-title" href= "/2018/01/23/重走android（9）View绘制流程/" >重走android（9）View绘制流程</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/22</span><a class="archive-post-title" href= "/2018/01/22/设计模式（2）工厂模式/" >设计模式（2）工厂模式</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/22</span><a class="archive-post-title" href= "/2018/01/22/重走android（8）IntentService/" >重走android（8）IntentService</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/22</span><a class="archive-post-title" href= "/2018/01/22/重走android（7）HandlerThread/" >重走android（7）HandlerThread</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/20</span><a class="archive-post-title" href= "/2018/01/20/重走android（6）Handler-MessageQueue-Looper/" >重走android（6）Handler,MessageQueue,Looper</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/17</span><a class="archive-post-title" href= "/2018/01/17/重走android（4）LocalBroadcastManager源码解析/" >重走android（4）LocalBroadcastManager源码解析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/17</span><a class="archive-post-title" href= "/2018/01/17/ConnectionManager理解/" >ConnectionManager理解</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/17</span><a class="archive-post-title" href= "/2018/01/17/重走android（5）广播实践/" >重走android（5）广播实践</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/17</span><a class="archive-post-title" href= "/2018/01/17/重走android（3）广播/" >重走android（3）广播</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/17</span><a class="archive-post-title" href= "/2018/01/17/重走android（2）服务/" >重走android（2）服务</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span><a class="archive-post-title" href= "/2018/01/16/启动activity随想/" >启动activity随想</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span><a class="archive-post-title" href= "/2018/01/16/设计模式（1）单例模式/" >设计模式（1）单例模式</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span><a class="archive-post-title" href= "/2018/01/16/重走android（0）前言/" >重走android（0）前言</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span><a class="archive-post-title" href= "/2018/01/16/重走android（1）活动启动模式/" >重走android（1）活动启动模式</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/04</span><a class="archive-post-title" href= "/2018/01/04/String-intern-方法/" >String intern()方法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/04</span><a class="archive-post-title" href= "/2018/01/04/java内存区域与内存溢出异常/" >java内存区域与内存溢出异常</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/03</span><a class="archive-post-title" href= "/2018/01/03/贝贝网实习面试（1）/" >贝贝网实习面试（1）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/03</span><a class="archive-post-title" href= "/2018/01/03/java-基础（1）/" >java 基础（1）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/03</span><a class="archive-post-title" href= "/2018/01/03/my-first-post/" >my first post</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">String</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">java</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">tip</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">随想</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">android</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">基础</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">java核心技术卷1（10版）</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">jvm</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">《深入理解Java虚拟机》</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">activity</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">考研</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">感想</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">论文</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">物联网</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">情景感知计算</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">机器学习</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">学习笔记</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">毕设</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">框架</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">线性代数</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">统计学习方法</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">练习</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">设计模式</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">面试</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">工作</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">重走android</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">源码</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ统计  -->
    
    <div style="display: none">
        <script src="https://s13.cnzz.com/z_stat.php?id=true&web_id=true" language="JavaScript"></script>
        
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>


